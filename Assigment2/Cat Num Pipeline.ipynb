{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6003da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('8k_diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5998fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight, payer_code, medical_specialty because they have a large number of missing values\n",
    "\n",
    "df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fabb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acetohexamide,examide,citoglipton,Troglitazone, glimepiride.pioglitazone, metformin.rosiglitazone ,metformin.pioglitazone \n",
    "\n",
    "#In the following columns, there are 2 or 3 classes, however one of them has the majority of values\n",
    "\n",
    "#Tolbutamide, glipizide.metformin \n",
    "#- The great majority of the data is NO, 7998. Only 2 are Steady\n",
    "df = df.drop(['tolbutamide', 'glipizide.metformin'], axis = 1)\n",
    "#Tolazamide\n",
    "#- The great majority of the data is NO, 7999. Only 1 is Steady\n",
    "df = df.drop(['tolazamide'], axis = 1)\n",
    "#Miglitol\n",
    "#- The great majority is NO 7997, Steady 2 and Down 1\n",
    "df = df.drop(['miglitol'], axis = 1)\n",
    "\n",
    "#Acarbose\n",
    "#- The majority is NO 7976, Steady 23 and Up 1\n",
    "df = df.drop(['acarbose'], axis = 1)\n",
    "\n",
    "#Chlorpropamide\n",
    "#- No 7990, Steady 9, Up 1\n",
    "df = df.drop(['chlorpropamide'], axis = 1)\n",
    "\n",
    "#Nateglinide\n",
    "#- No 7962, Steady 36, Down 1, Up 1\n",
    "df = df.drop(['nateglinide'], axis = 1)\n",
    "\n",
    "#Repaglinide\n",
    "#- No 7888, Steady 96, Up 11, Down 5\n",
    "df = df.drop(['repaglinide'], axis = 1)\n",
    "\n",
    "#acetohexamide has only one value for all rows\n",
    "df = df.drop(['acetohexamide'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5eb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explain why I use Not Mapped instead of ?\n",
    "\n",
    "df = df.replace('?', 'Not Mapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080250e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['admission_type_id'].isnull().sum()\n",
    "\n",
    "# I decided to replace the null values in the following columns because the data on these rows could be important\n",
    "\n",
    "df['admission_type_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "df['discharge_disposition_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "df['admission_source_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "\n",
    "# In the case of the diagnostic description, I prefer to leave it as \"\" but not null. \n",
    "df['diag_2_desc'].fillna(\"\", inplace = True)\n",
    "df['diag_3_desc'].fillna(\"\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d4c0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].replace({\"[70-80)\":75,\n",
    "                         \"[60-70)\":65,\n",
    "                         \"[50-60)\":55,\n",
    "                         \"[80-90)\":85,\n",
    "                         \"[40-50)\":45,\n",
    "                         \"[30-40)\":35,\n",
    "                         \"[90-100)\":95,\n",
    "                         \"[20-30)\":25,\n",
    "                         \"[10-20)\":15,\n",
    "                         \"[0-10)\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c41812c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from time import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean_text is a function to remove tokens like white spaces in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41bbc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_diag_1_desc'] = df.diag_1_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "df['Processed_diag_2_desc'] = df.diag_2_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "df['Processed_diag_3_desc'] = df.diag_3_desc.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dccd2d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 41) (8000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 ColumnTransformer(transformers=[('cat_encoder',\n",
       "                                                  CatBoostEncoder(),\n",
       "                                                  Index(['race', 'gender', 'age', 'admission_type_id',\n",
       "       'discharge_disposition_id', 'admission_source_id', 'diag_1', 'diag_2',\n",
       "       'diag_3', 'max_glu_serum', 'A1Cresult', 'metformin', 'glimepiride',\n",
       "       'glipizide', 'glyburide', 'pioglitazone', 'rosiglitazone',\n",
       "       'troglitazone', 'examide', 'c...\n",
       "       'glyburide.metformin', 'glimepiride.pioglitazone',\n",
       "       'metformin.rosiglitazone', 'metformin.pioglitazone', 'change',\n",
       "       'diabetesMed'],\n",
       "      dtype='object')),\n",
       "                                                 ('num_scaler', MinMaxScaler(),\n",
       "                                                  Index(['age', 'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
       "       'num_medications', 'number_outpatient', 'number_emergency',\n",
       "       'number_inpatient', 'number_diagnoses'],\n",
       "      dtype='object'))]))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# example of using the ColumnTransformer for the Abalone dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# split into inputs and outputs\n",
    "\n",
    "X = df.drop(['readmitted'],axis =1)\n",
    "y = df['readmitted'].astype(int)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# determine categorical and numerical features\n",
    "\n",
    "numerical_columns = X.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns\n",
    "\n",
    "categorical_columns = df.drop(['time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc','Processed_diag_1_desc','Processed_diag_2_desc','Processed_diag_3_desc','readmitted'],axis=1).columns\n",
    "#X.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "text_columns = ['Processed_diag_1_desc','Processed_diag_2_desc','Processed_diag_3_desc']\n",
    "\n",
    "# define the data preparation for the columns\n",
    "\n",
    "cat_encoder = ce.CatBoostEncoder()\n",
    "\n",
    "tfidf_params = dict(sublinear_tf= True, \n",
    "                       #min_df = 5, \n",
    "                       norm= 'l2', \n",
    "                       #ngram_range= (1,2), \n",
    "                       stop_words ='english')\n",
    "\n",
    "transformer = [\n",
    "    ('cat_encoder', cat_encoder, categorical_columns), \n",
    "    #('num_scaler', MinMaxScaler(), numerical_columns)\n",
    "    ('num_scaler', MinMaxScaler(), numerical_columns),\n",
    "    #('tf_idf',TfidfVectorizer(**tfidf_params), text_columns)\n",
    "    ]\n",
    "\n",
    "col_transform = ColumnTransformer(transformers=transformer)\n",
    "\n",
    "#text_model = LogisticRegression(solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5)\n",
    "\n",
    "pipeline = Pipeline(steps=[('prep',col_transform)])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35c4e31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text_model',\n",
       "  LogisticRegression(l1_ratio=0.5, penalty='elasticnet', solver='saga'),\n",
       "  ['Processed_diag_1_desc', 'Processed_diag_2_desc', 'Processed_diag_3_desc'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model_transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
