{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f8a4f",
   "metadata": {},
   "source": [
    "### Perform Exploratory Data Analysis (EDA) and discuss the data and what you observe prior to beginning modeling and how impact how to proceed ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd70850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 51)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('8k_diabetes.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47a066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "df.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the counts and Data types of each column \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166cd3d",
   "metadata": {},
   "source": [
    "On the previous step, I discovered that these columns contain null values.\n",
    "\n",
    "admission_type_id\n",
    "discharge_disposition_id \n",
    "admission_source_id\n",
    "diag_2_desc\n",
    "diag_3_desc\n",
    "\n",
    "The null values from these columns can be replaced with '?', which is the simbol for 'Missing Values' in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values in all the columns \n",
    "# Here I want to see the number of nulls.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop will print the unique classes of each column \n",
    "\n",
    "print(\"All classes by column\")\n",
    "for column in df.columns:\n",
    "    print(column)\n",
    "    print(df[column].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb54da9",
   "metadata": {},
   "source": [
    "The age column can be considered categorical data since the values represent a range of age. \n",
    "Maybe the values could be replaced with an age in the middle, for example 50-60 = 55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaaf55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This loop will print the counts of each class in every column\n",
    "\n",
    "print(\"All value counts by column\")\n",
    "for column in df.columns:\n",
    "    print(column,\"\\n\")\n",
    "    \n",
    "    print(df[column].value_counts(),\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164322d5",
   "metadata": {},
   "source": [
    "In the dataset '?' represents mission values.\n",
    "This code will show the count of '?' in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a87e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race 180\n",
      "gender 0\n",
      "age 0\n",
      "weight 7675\n",
      "admission_type_id 0\n",
      "discharge_disposition_id 0\n",
      "admission_source_id 0\n",
      "payer_code 4260\n",
      "medical_specialty 3229\n",
      "diag_1 0\n",
      "diag_2 55\n",
      "diag_3 175\n",
      "max_glu_serum 0\n",
      "A1Cresult 0\n",
      "metformin 0\n",
      "repaglinide 0\n",
      "nateglinide 0\n",
      "chlorpropamide 0\n",
      "glimepiride 0\n",
      "acetohexamide 0\n",
      "glipizide 0\n",
      "glyburide 0\n",
      "tolbutamide 0\n",
      "pioglitazone 0\n",
      "rosiglitazone 0\n",
      "acarbose 0\n",
      "miglitol 0\n",
      "troglitazone 0\n",
      "tolazamide 0\n",
      "examide 0\n",
      "citoglipton 0\n",
      "insulin 0\n",
      "glyburide.metformin 0\n",
      "glipizide.metformin 0\n",
      "glimepiride.pioglitazone 0\n",
      "metformin.rosiglitazone 0\n",
      "metformin.pioglitazone 0\n",
      "change 0\n",
      "diabetesMed 0\n",
      "diag_1_desc 0\n",
      "diag_2_desc 0\n",
      "diag_3_desc 0\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "         print(col,df[col][df[col] == '?'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f1172",
   "metadata": {},
   "source": [
    "### Pre-processed categorical data for use in the model and justified pre-processing method. Note this may be different for each algorithm you try ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb98513",
   "metadata": {},
   "source": [
    "Drop the columns with a large count of missing values and where the majority of categories are only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5d0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight, payer_code, medical_specialty because they have a large number of missing values\n",
    "\n",
    "df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462b9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acetohexamide,examide,citoglipton,Troglitazone, glimepiride.pioglitazone, metformin.rosiglitazone ,metformin.pioglitazone \n",
    "\n",
    "#In the following columns, there are 2 or 3 classes, however one of them has the majority of values\n",
    "\n",
    "#Tolbutamide, glipizide.metformin \n",
    "#- The great majority of the data is NO, 7998. Only 2 are Steady\n",
    "df = df.drop(['tolbutamide', 'glipizide.metformin'], axis = 1)\n",
    "#Tolazamide\n",
    "#- The great majority of the data is NO, 7999. Only 1 is Steady\n",
    "df = df.drop(['tolazamide'], axis = 1)\n",
    "#Miglitol\n",
    "#- The great majority is NO 7997, Steady 2 and Down 1\n",
    "df = df.drop(['miglitol'], axis = 1)\n",
    "\n",
    "#Acarbose\n",
    "#- The majority is NO 7976, Steady 23 and Up 1\n",
    "df = df.drop(['acarbose'], axis = 1)\n",
    "\n",
    "#Chlorpropamide\n",
    "#- No 7990, Steady 9, Up 1\n",
    "df = df.drop(['chlorpropamide'], axis = 1)\n",
    "\n",
    "#Nateglinide\n",
    "#- No 7962, Steady 36, Down 1, Up 1\n",
    "df = df.drop(['nateglinide'], axis = 1)\n",
    "\n",
    "#Repaglinide\n",
    "#- No 7888, Steady 96, Up 11, Down 5\n",
    "df = df.drop(['repaglinide'], axis = 1)\n",
    "\n",
    "#acetohexamide has only one value for all rows\n",
    "df = df.drop(['acetohexamide'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d45d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where discharge_disposition_id = expired, because it means that the patient died.\n",
    "\n",
    "df = df.drop(df[df.discharge_disposition_id =='Expired'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420c4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719fcfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', 'Not Mapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682524dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['admission_type_id'].isnull().sum()\n",
    "\n",
    "# I decided to replace the null values in the following columns because the data on these rows could be important\n",
    "\n",
    "df['admission_type_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "df['discharge_disposition_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "df['admission_source_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "\n",
    "# In the case of the diagnostic description, I prefer to leave it as \"\" but not null. \n",
    "df['diag_2_desc'].fillna(\"\", inplace = True)\n",
    "df['diag_3_desc'].fillna(\"\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244649c3",
   "metadata": {},
   "source": [
    "### Pre-processed numerical data appropriately including handling missing data and justified methods used. Note this may be different for each algorithm you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab390a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>time_in_hospital</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_procedures</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_medications</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_outpatient</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_emergency</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_inpatient</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_diagnoses</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0   1   2   3   4   5   7   8   9   10\n",
       "time_in_hospital     1   2   7   4   5   4   2   3   5  14\n",
       "num_lab_procedures  35   8  12  33  31  29  49  54  47  45\n",
       "num_procedures       4   5   0   1   0   0   1   0   2   2\n",
       "num_medications     21   5  21   5  13  10  17  10  12  44\n",
       "number_outpatient    0   0   0   0   0   0   2   0   0   0\n",
       "number_emergency     0   0   0   0   0   0   1   0   0   0\n",
       "number_inpatient     0   0   1   0   0   0   1   1   0   0\n",
       "number_diagnoses     9   6   9   3   7   8   9   9   5   9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will return all the numeric columns\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df.select_dtypes(include=numerics)[:10].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc57bb3",
   "metadata": {},
   "source": [
    "Apparently, there are no missing values for the previously listed columns. However, there are some columns like age that could be transformed to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7378baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].replace({\"[70-80)\":75,\n",
    "                         \"[60-70)\":65,\n",
    "                         \"[50-60)\":55,\n",
    "                         \"[80-90)\":85,\n",
    "                         \"[40-50)\":45,\n",
    "                         \"[30-40)\":35,\n",
    "                         \"[90-100)\":95,\n",
    "                         \"[20-30)\":25,\n",
    "                         \"[10-20)\":15,\n",
    "                         \"[0-10)\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f3ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode columns with categorical data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "cat_data=cat_data = df.drop(['time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc','readmitted'], axis = 1 )\n",
    "\n",
    "for i in cat_data:\n",
    "    cat_data[i] = le.fit_transform(cat_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aead1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"readmitted\"] = df[\"readmitted\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e00922",
   "metadata": {},
   "source": [
    "### Implement a model to make predictions using text data using tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60170c",
   "metadata": {},
   "source": [
    "Before creating the model, the text data should be processed. I decided to use Lemmatization and Stop Words elimination.\n",
    "Lemmatization: group words and use the lemma (base form), removing the endings of each word.\n",
    "Stop Words: Remove words that don't have a significative meaning. (sometimes words used as connectors like \"the\", \"a\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12240764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from time import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f4b2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean_text is a function to remove tokens like white spaces in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d740c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the stop words from the text\n",
    "df['Processed_diag_1_desc'] = df.diag_1_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "df['Processed_diag_2_desc'] = df.diag_2_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "df['Processed_diag_3_desc'] = df.diag_3_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "# Create a new column joining the 3 text columns\n",
    "\n",
    "df['diag_desc'] = df['diag_1_desc'] + '. ' + df['diag_2_desc'] + '. ' + df['diag_3_desc']\n",
    "\n",
    "df['Processed_diag_desc'] = df.diag_desc.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae2afc",
   "metadata": {},
   "source": [
    "### Para esta parte del deber, usar el pipeline siguiente para crear 3 modelos con texto:\n",
    "\n",
    "\n",
    "\n",
    "1) Un modelo con Processed_diag_1_desc\n",
    "\n",
    "2) Un modelo con Processed_diag_2_desc\n",
    "\n",
    "3) Un modelo con Processed_diag_3_desc\n",
    "\n",
    "4) Un modelo solo con datos categoricos (Es posible, esta listo... practicamente es lo de abajo)\n",
    "\n",
    "5) Usar la salida de todos los modelos en model stacking\n",
    "\n",
    "\n",
    "Nota: El modelo siguiente funciona con datos categoricos. UTILIZAR EL MODELO DEL OTRO ARCHIVO PARA TEXTO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f90af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['Processed_diag_1_desc','Processed_diag_2_desc','Processed_diag_3_desc']], df['readmitted'], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976db42",
   "metadata": {},
   "source": [
    "#### Applying TfidfVectorizer (This part works for the text models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a947c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the params\n",
    "tfidf_params = dict(sublinear_tf= True, \n",
    "                       #min_df = 5, \n",
    "                       norm= 'l2', \n",
    "                       #ngram_range= (1,2), \n",
    "                       stop_words ='english')\n",
    "\n",
    "\n",
    "# create a Pipeline that will do features transformation then pass to the model\n",
    "\n",
    "cls = Pipeline(steps=[\n",
    "('features', TfidfVectorizer(**tfidf_params)),\n",
    "#('elasticnet', ElasticNet(random_state=0))\n",
    "('clf', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "256ce6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['Processed_diag_1_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred_model1 = cls.predict(X_test['Processed_diag_1_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2c18c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['Processed_diag_2_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred_model2 = cls.predict(X_test['Processed_diag_2_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34db59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['Processed_diag_3_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred_model3 = cls.predict(X_test['Processed_diag_3_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc412c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.609072375127421\n",
      "Precision: 0.5075075075075075\n",
      "Recall: 0.2189119170984456\n",
      "F1-Score: 0.3058823529411765\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\",accuracy_score(y_test,y_pred_model3))\n",
    "print('Precision:',precision_score(y_test,y_pred_model3))\n",
    "print('Recall:',recall_score(y_test,y_pred_model3))\n",
    "print('F1-Score:',f1_score(y_test,y_pred_model3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce65e51",
   "metadata": {},
   "source": [
    "#### Categorical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc52393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for text data\n",
    "text_features = ['Processed_diag_desc']\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "categorical_features = [\n",
    "    'race', \n",
    "    'gender',\n",
    "    'age',\n",
    "    'admission_type_id',\n",
    "    'discharge_disposition_id',\n",
    "    'admission_source_id',\n",
    "    'diag_1',\n",
    "    'diag_2',\n",
    "    'diag_3',\n",
    "    'max_glu_serum',\n",
    "    'A1Cresult',\n",
    "    'metformin',\n",
    "    'glimepiride',\n",
    "    'glipizide',\n",
    "    'glyburide', \n",
    "    'pioglitazone', \n",
    "    'rosiglitazone',\n",
    "    'troglitazone', \n",
    "    'examide',\n",
    "    'citoglipton',\n",
    "    'insulin',\n",
    "    'glyburide.metformin',\n",
    "    'glimepiride.pioglitazone',\n",
    "    'metformin.rosiglitazone', \n",
    "    'metformin.pioglitazone', \n",
    "    'change',\n",
    "    'diabetesMed']\n",
    "\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# you can add other transformations for other data types\n",
    "\n",
    "# combine preprocessing with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        #('text', text_transformer, text_features)\n",
    "])\n",
    "\n",
    "## Model Part\n",
    "#clf = Pipeline(\n",
    "#    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    "#)\n",
    "\n",
    "# Split the data\n",
    "\n",
    "X = df.drop(['readmitted'], axis = 1)\n",
    "\n",
    "y = df['readmitted']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# add model to be part of pipeline\n",
    "clf_pipe =  Pipeline(steps=[#(\"preprocessor\", preprocessor),\n",
    "                            #(\"mNB\", MultinomialNB(alpha=1)),\n",
    "                            #(\"svm\", SVC()),\n",
    "                            (\"rfc\", RandomForestClassifier(n_estimators=1000, random_state=0)),                                                        \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe35d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipe.fit(X_train, y_train)\n",
    "clf_pipe.predict(X_test)\n",
    "\n",
    "clf_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46585abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
