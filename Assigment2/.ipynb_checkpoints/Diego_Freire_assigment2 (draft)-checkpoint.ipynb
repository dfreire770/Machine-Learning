{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f8a4f",
   "metadata": {},
   "source": [
    "### Perform Exploratory Data Analysis (EDA) and discuss the data and what you observe prior to beginning modeling and how impact how to proceed ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecd70850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 51)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('8k_diabetes.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47a066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "df.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the counts and Data types of each column \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166cd3d",
   "metadata": {},
   "source": [
    "On the previous step, I discovered that these columns contain null values.\n",
    "\n",
    "admission_type_id\n",
    "discharge_disposition_id \n",
    "admission_source_id\n",
    "diag_2_desc\n",
    "diag_3_desc\n",
    "\n",
    "The null values from these columns can be replaced with '?', which is the simbol for 'Missing Values' in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values in all the columns \n",
    "# Here I want to see the number of nulls.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop will print the unique classes of each column \n",
    "\n",
    "print(\"All classes by column\")\n",
    "for column in df.columns:\n",
    "    print(column)\n",
    "    print(df[column].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb54da9",
   "metadata": {},
   "source": [
    "The age column can be considered categorical data since the values represent a range of age. \n",
    "Maybe the values could be replaced with an age in the middle, for example 50-60 = 55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaaf55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This loop will print the counts of each class in every column\n",
    "\n",
    "print(\"All value counts by column\")\n",
    "for column in df.columns:\n",
    "    print(column,\"\\n\")\n",
    "    \n",
    "    print(df[column].value_counts(),\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164322d5",
   "metadata": {},
   "source": [
    "In the dataset '?' represents mission values.\n",
    "This code will show the count of '?' in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "         print(col,df[col][df[col] == '?'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f1172",
   "metadata": {},
   "source": [
    "### Pre-processed categorical data for use in the model and justified pre-processing method. Note this may be different for each algorithm you try ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb98513",
   "metadata": {},
   "source": [
    "Drop the columns with a large count of missing values and where the majority of categories are only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d5d0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight, payer_code, medical_specialty because they have a large number of missing values\n",
    "\n",
    "df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "462b9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acetohexamide,examide,citoglipton,Troglitazone, glimepiride.pioglitazone, metformin.rosiglitazone ,metformin.pioglitazone \n",
    "\n",
    "#In the following columns, there are 2 or 3 classes, however one of them has the majority of values\n",
    "\n",
    "#Tolbutamide, glipizide.metformin \n",
    "#- The great majority of the data is NO, 7998. Only 2 are Steady\n",
    "df = df.drop(['tolbutamide', 'glipizide.metformin'], axis = 1)\n",
    "#Tolazamide\n",
    "#- The great majority of the data is NO, 7999. Only 1 is Steady\n",
    "df = df.drop(['tolazamide'], axis = 1)\n",
    "#Miglitol\n",
    "#- The great majority is NO 7997, Steady 2 and Down 1\n",
    "df = df.drop(['miglitol'], axis = 1)\n",
    "\n",
    "#Acarbose\n",
    "#- The majority is NO 7976, Steady 23 and Up 1\n",
    "df = df.drop(['acarbose'], axis = 1)\n",
    "\n",
    "#Chlorpropamide\n",
    "#- No 7990, Steady 9, Up 1\n",
    "df = df.drop(['chlorpropamide'], axis = 1)\n",
    "\n",
    "#Nateglinide\n",
    "#- No 7962, Steady 36, Down 1, Up 1\n",
    "df = df.drop(['nateglinide'], axis = 1)\n",
    "\n",
    "#Repaglinide\n",
    "#- No 7888, Steady 96, Up 11, Down 5\n",
    "df = df.drop(['repaglinide'], axis = 1)\n",
    "\n",
    "#acetohexamide has only one value for all rows\n",
    "df = df.drop(['acetohexamide'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86d45d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where discharge_disposition_id = expired, because it means that the patient died.\n",
    "\n",
    "df = df.drop(df[df.discharge_disposition_id =='Expired'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420c4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "682524dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['admission_type_id'].isnull().sum()\n",
    "\n",
    "# I decided to replace the null values in the following columns because the data on these rows could be important\n",
    "\n",
    "df['admission_type_id'].fillna(\"?\", inplace = True)\n",
    "df['discharge_disposition_id'].fillna(\"?\", inplace = True)\n",
    "df['admission_source_id'].fillna(\"?\", inplace = True)\n",
    "\n",
    "# In the case of the diagnostic description, I prefer to leave it as \"\" but not null. \n",
    "df['diag_2_desc'].fillna(\"\", inplace = True)\n",
    "df['diag_3_desc'].fillna(\"\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode columns with categorical data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "cat_data=cat_data = df.drop(['time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc'], axis = 1 )\n",
    "\n",
    "for i in cat_data:\n",
    "    cat_data[i] = le.fit_transform(cat_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244649c3",
   "metadata": {},
   "source": [
    "### Pre-processed numerical data appropriately including handling missing data and justified methods used. Note this may be different for each algorithm you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab390a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will return all the numeric columns\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df.select_dtypes(include=numerics)[:10].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc57bb3",
   "metadata": {},
   "source": [
    "Apparently, there are no missing values for the previously listed columns. However, there are some columns like age that could be transformed to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7378baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].replace({\"[70-80)\":75,\n",
    "                         \"[60-70)\":65,\n",
    "                         \"[50-60)\":55,\n",
    "                         \"[80-90)\":85,\n",
    "                         \"[40-50)\":45,\n",
    "                         \"[30-40)\":35,\n",
    "                         \"[90-100)\":95,\n",
    "                         \"[20-30)\":25,\n",
    "                         \"[10-20)\":15,\n",
    "                         \"[0-10)\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aead1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"readmitted\"] = df[\"readmitted\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e00922",
   "metadata": {},
   "source": [
    "### Implement a model to make predictions using text data using tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60170c",
   "metadata": {},
   "source": [
    "Before creating the model, the text data should be processed. I decided to use Lemmatization and Stop Words elimination.\n",
    "Lemmatization: group words and use the lemma (base form), removing the endings of each word.\n",
    "Stop Words: Remove words that don't have a significative meaning. (sometimes words used as connectors like \"the\", \"a\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12240764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from time import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f4b2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean_text is a function to remove tokens like white spaces in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d740c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the stop words from the text\n",
    "df['Processed_diag_1_desc'] = df.diag_1_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "df['Processed_diag_2_desc'] = df.diag_2_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "df['Processed_diag_3_desc'] = df.diag_3_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "# Create a new column joining the 3 text columns\n",
    "\n",
    "df['diag_desc'] = df['diag_1_desc'] + '. ' + df['diag_2_desc'] + '. ' + df['diag_3_desc']\n",
    "\n",
    "df['Processed_diag_desc'] = df.diag_desc.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976db42",
   "metadata": {},
   "source": [
    "#### Applying TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dbd14490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect=TfidfVectorizer()\n",
    "\n",
    "diag_1_tfidf=tfidf_vect.fit_transform(df['Processed_diag_1_desc'].values.tolist())\n",
    "diag_2_tfidf=tfidf_vect.fit_transform(df['Processed_diag_2_desc'].values.tolist())\n",
    "diag_3_tfidf=tfidf_vect.fit_transform(df['Processed_diag_3_desc'].values.tolist())\n",
    "\n",
    "diag_desc_tfidf  = tfidf_vect.fit_transform(df['Processed_diag_desc'].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9b27327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diag_desc_tfidf\n",
    "\n",
    "y = df['readmitted']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce65e51",
   "metadata": {},
   "source": [
    "### These models will only use the text data (diag_1_desc, diag_2_desc, diag_3_desc combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a57908",
   "metadata": {},
   "source": [
    "#### Using Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3d99bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a model using Naive Bayes Classifier using the combined diag_descript tfidf weights\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "nb_y_pred = naive_bayes_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c7fdcedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.780891719745223\n",
      "Precision: 0.46\n",
      "Recall: 0.3021346469622332\n",
      "F1-Score: 0.36471754212091184\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\",accuracy_score(y_pred, nb_y_pred))\n",
    "print('Precision:',precision_score(y_test, nb_y_pred))\n",
    "print('Recall:',recall_score(y_test, nb_y_pred))\n",
    "print('F1-Score:',f1_score(y_test, nb_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d72b008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5614200109696899"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate performance with AUC\n",
    "\n",
    "roc_auc_score(y_test, naive_bayes_classifier.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# This model is not good enough. Notice that this is only using the text portion of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b62627",
   "metadata": {},
   "source": [
    "#### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f7c7ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', probability=True)\n",
    "\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "y_pred_SVM = SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2bb04d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5987261146496815\n",
      "Precision: 0.45023696682464454\n",
      "Recall: 0.15599343185550082\n",
      "F1-Score: 0.23170731707317072\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\",accuracy_score(y_test,y_pred_SVM))\n",
    "print('Precision:',precision_score(y_test, y_pred_SVM))\n",
    "print('Recall:',recall_score(y_test, y_pred_SVM))\n",
    "print('F1-Score:',f1_score(y_test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1a170b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5645878933582116"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, SVM.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e39950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "664fb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6798    1\n",
      "2205    1\n",
      "3484    1\n",
      "1941    0\n",
      "1483    0\n",
      "       ..\n",
      "4724    0\n",
      "4123    0\n",
      "3637    0\n",
      "3759    0\n",
      "617     0\n",
      "Name: readmitted, Length: 2354, dtype: int32 [0 1 0 ... 1 0 0]\n",
      "Score of Naive Bayes is : 0.5909090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1298: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "('vect', CountVectorizer(stop_words='english',lowercase=True)),\n",
    "(\"tfidf1\", TfidfTransformer(use_idf=True,smooth_idf=True)),\n",
    "('clf', MultinomialNB(alpha=1)) #Laplace smoothing\n",
    " ])\n",
    "\n",
    "train,test=train_test_split(df,test_size=.3,random_state=42, shuffle=True)\n",
    "pipeline.fit(train['diag_desc'],train['readmitted'])\n",
    "\n",
    "predictions=pipeline.predict(test['diag_desc'])\n",
    "print(test['readmitted'],predictions)\n",
    "\n",
    "score = f1_score(test['readmitted'],predictions,pos_label='positive',average='micro')\n",
    "print(\"Score of Naive Bayes is :\" , score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "24276e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5909090909090909\n",
      "Precision: 0.4940374787052811\n",
      "Recall: 0.303347280334728\n",
      "F1-Score: 0.37589112119248214\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\",accuracy_score(test['readmitted'],predictions))\n",
    "print('Precision:',precision_score(test['readmitted'],predictions))\n",
    "print('Recall:',recall_score(test['readmitted'],predictions))\n",
    "print('F1-Score:',f1_score(test['readmitted'],predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2314a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6012353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "325bc67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5624203821656051\n",
      "Precision: 0.40930232558139534\n",
      "Recall: 0.2889983579638752\n",
      "F1-Score: 0.3387872954764196\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\",accuracy_score(y_test,y_pred))\n",
    "print('Precision:',precision_score(y_test,y_pred))\n",
    "print('Recall:',recall_score(y_test,y_pred))\n",
    "print('F1-Score:',f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c03b2d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5358001466042659"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b505c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d227afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "     transformers=[\n",
    "         ('text', TfidfVectorizer(), 'raw_text_ft'), #TfidfVectorizer accepts column name only between quotes\n",
    "         ('category', LabelEncoder(), ['categorical_ft']),\n",
    "     ],\n",
    ")\n",
    "pipe = Pipeline(\n",
    "     steps=[\n",
    "         ('preprocessor', preprocessor),\n",
    "         ('classifier', LogisticRegression()),\n",
    "     ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b30f06cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_58416/3348223556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'diag_desc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'readmitted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# set n_features_in_ attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    350\u001b[0m                \u001b[0mshould\u001b[0m \u001b[0mset\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "pipe.fit(train['diag_desc'],train['readmitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc52393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
