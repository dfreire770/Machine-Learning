{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d40748d",
   "metadata": {},
   "source": [
    "### Perform Exploratory Data Analysis (EDA) and discuss the data and what you observe prior to beginning modeling and how impact how to proceed ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f208de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('8k_diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a8168",
   "metadata": {},
   "source": [
    "Explore the data and explain why I don't need all the columns should be included\n",
    "\n",
    "I also need charts to show correlations, and to help understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa34fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight, payer_code, medical_specialty because they have a large number of missing values\n",
    "\n",
    "df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e19b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acetohexamide,examide,citoglipton,Troglitazone, glimepiride.pioglitazone, metformin.rosiglitazone ,metformin.pioglitazone \n",
    "\n",
    "#In the following columns, there are 2 or 3 classes, however one of them has the majority of values\n",
    "\n",
    "#Tolbutamide, glipizide.metformin \n",
    "#- The great majority of the data is NO, 7998. Only 2 are Steady\n",
    "df = df.drop(['tolbutamide', 'glipizide.metformin'], axis = 1)\n",
    "#Tolazamide\n",
    "#- The great majority of the data is NO, 7999. Only 1 is Steady\n",
    "df = df.drop(['tolazamide'], axis = 1)\n",
    "#Miglitol\n",
    "#- The great majority is NO 7997, Steady 2 and Down 1\n",
    "df = df.drop(['miglitol'], axis = 1)\n",
    "\n",
    "#Acarbose\n",
    "#- The majority is NO 7976, Steady 23 and Up 1\n",
    "df = df.drop(['acarbose'], axis = 1)\n",
    "\n",
    "#Chlorpropamide\n",
    "#- No 7990, Steady 9, Up 1\n",
    "df = df.drop(['chlorpropamide'], axis = 1)\n",
    "\n",
    "#Nateglinide\n",
    "#- No 7962, Steady 36, Down 1, Up 1\n",
    "df = df.drop(['nateglinide'], axis = 1)\n",
    "\n",
    "#Repaglinide\n",
    "#- No 7888, Steady 96, Up 11, Down 5\n",
    "df = df.drop(['repaglinide'], axis = 1)\n",
    "\n",
    "#acetohexamide has only one value for all rows\n",
    "df = df.drop(['acetohexamide'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068b60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where discharge_disposition_id = expired, because it means that the patient died.\n",
    "\n",
    "### Actually, check if this makes sense.\n",
    "\n",
    "df = df.drop(df[df.discharge_disposition_id =='Expired'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054a5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explain why I use Not Mapped instead of ?\n",
    "\n",
    "df = df.replace('?', 'Not Mapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c3fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['admission_type_id'].isnull().sum()\n",
    "\n",
    "# I decided to replace the null values in the following columns because the data on these rows could be important\n",
    "\n",
    "df['admission_type_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "df['discharge_disposition_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "df['admission_source_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "\n",
    "# In the case of the diagnostic description, I prefer to leave it as \"\" but not null. \n",
    "df['diag_2_desc'].fillna(\"\", inplace = True)\n",
    "df['diag_3_desc'].fillna(\"\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4c530",
   "metadata": {},
   "source": [
    "### Pre-processed categorical data for use in the model and justified pre-processing method. Note this may be different for each algorithm you try ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc03cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.CatBoostEncoder()\n",
    "\n",
    "cat_data=cat_data = df.drop(['age','time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc','readmitted'], axis = 1 )\n",
    "\n",
    "target = df[\"readmitted\"].astype(int)\n",
    "\n",
    "encoder.fit(cat_data, target)\n",
    "\n",
    "cat_data = encoder.transform(cat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d07a258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>diag_1</th>\n",
       "      <th>diag_2</th>\n",
       "      <th>diag_3</th>\n",
       "      <th>max_glu_serum</th>\n",
       "      <th>A1Cresult</th>\n",
       "      <th>...</th>\n",
       "      <th>troglitazone</th>\n",
       "      <th>examide</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide.metformin</th>\n",
       "      <th>glimepiride.pioglitazone</th>\n",
       "      <th>metformin.rosiglitazone</th>\n",
       "      <th>metformin.pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.378359</td>\n",
       "      <td>0.393526</td>\n",
       "      <td>0.388637</td>\n",
       "      <td>0.045005</td>\n",
       "      <td>0.067508</td>\n",
       "      <td>0.340505</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.391587</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.380574</td>\n",
       "      <td>0.355984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>0.393526</td>\n",
       "      <td>0.388637</td>\n",
       "      <td>0.081009</td>\n",
       "      <td>0.133486</td>\n",
       "      <td>0.450055</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.391587</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.380574</td>\n",
       "      <td>0.355984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.404097</td>\n",
       "      <td>0.350011</td>\n",
       "      <td>0.504277</td>\n",
       "      <td>0.421243</td>\n",
       "      <td>0.309550</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>0.387448</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.380574</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.347025</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.407311</td>\n",
       "      <td>0.393526</td>\n",
       "      <td>0.226778</td>\n",
       "      <td>0.428426</td>\n",
       "      <td>0.246835</td>\n",
       "      <td>0.320319</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.380574</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347025</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.407311</td>\n",
       "      <td>0.393526</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.300065</td>\n",
       "      <td>0.389710</td>\n",
       "      <td>0.081009</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.437398</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.404097</td>\n",
       "      <td>0.378359</td>\n",
       "      <td>0.439837</td>\n",
       "      <td>0.388637</td>\n",
       "      <td>0.467563</td>\n",
       "      <td>0.389739</td>\n",
       "      <td>0.510122</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.380574</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>0.393526</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.400841</td>\n",
       "      <td>0.407386</td>\n",
       "      <td>0.400459</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.437398</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.404097</td>\n",
       "      <td>0.407311</td>\n",
       "      <td>0.378376</td>\n",
       "      <td>0.421243</td>\n",
       "      <td>0.371294</td>\n",
       "      <td>0.375610</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>0.372304</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.437398</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.404097</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>0.439837</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.537561</td>\n",
       "      <td>0.416229</td>\n",
       "      <td>0.505330</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.453644</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.437398</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0.428444</td>\n",
       "      <td>0.405859</td>\n",
       "      <td>0.378359</td>\n",
       "      <td>0.504277</td>\n",
       "      <td>0.262436</td>\n",
       "      <td>0.451118</td>\n",
       "      <td>0.470407</td>\n",
       "      <td>0.437276</td>\n",
       "      <td>0.406701</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.392105</td>\n",
       "      <td>0.404768</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.405047</td>\n",
       "      <td>0.437398</td>\n",
       "      <td>0.421059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7846 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          race    gender  admission_type_id  discharge_disposition_id  \\\n",
       "0     0.428444  0.405859           0.378359                  0.393526   \n",
       "1     0.428444  0.405859           0.398159                  0.393526   \n",
       "2     0.428444  0.404097           0.350011                  0.504277   \n",
       "3     0.347025  0.405859           0.407311                  0.393526   \n",
       "4     0.347025  0.405859           0.407311                  0.393526   \n",
       "...        ...       ...                ...                       ...   \n",
       "7995  0.428444  0.404097           0.378359                  0.439837   \n",
       "7996  0.428444  0.405859           0.398159                  0.393526   \n",
       "7997  0.428444  0.404097           0.407311                  0.378376   \n",
       "7998  0.428444  0.404097           0.398159                  0.439837   \n",
       "7999  0.428444  0.405859           0.378359                  0.504277   \n",
       "\n",
       "      admission_source_id    diag_1    diag_2    diag_3  max_glu_serum  \\\n",
       "0                0.388637  0.045005  0.067508  0.340505       0.406701   \n",
       "1                0.388637  0.081009  0.133486  0.450055       0.406701   \n",
       "2                0.421243  0.309550  0.475400  0.447551       0.387448   \n",
       "3                0.226778  0.428426  0.246835  0.320319       0.406701   \n",
       "4                0.442550  0.300065  0.389710  0.081009       0.406701   \n",
       "...                   ...       ...       ...       ...            ...   \n",
       "7995             0.388637  0.467563  0.389739  0.510122       0.406701   \n",
       "7996             0.442550  0.400841  0.407386  0.400459       0.406701   \n",
       "7997             0.421243  0.371294  0.375610  0.447551       0.372304   \n",
       "7998             0.442550  0.537561  0.416229  0.505330       0.406701   \n",
       "7999             0.262436  0.451118  0.470407  0.437276       0.406701   \n",
       "\n",
       "      A1Cresult  ...  troglitazone   examide  citoglipton   insulin  \\\n",
       "0      0.410599  ...      0.405047  0.405047     0.405047  0.391587   \n",
       "1      0.410599  ...      0.405047  0.405047     0.405047  0.391587   \n",
       "2      0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "3      0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "4      0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "...         ...  ...           ...       ...          ...       ...   \n",
       "7995   0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "7996   0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "7997   0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "7998   0.410599  ...      0.405047  0.405047     0.405047  0.453644   \n",
       "7999   0.410599  ...      0.405047  0.405047     0.405047  0.392105   \n",
       "\n",
       "      glyburide.metformin  glimepiride.pioglitazone  metformin.rosiglitazone  \\\n",
       "0                0.404768                  0.405047                 0.405047   \n",
       "1                0.404768                  0.405047                 0.405047   \n",
       "2                0.404768                  0.405047                 0.405047   \n",
       "3                0.404768                  0.405047                 0.405047   \n",
       "4                0.404768                  0.405047                 0.405047   \n",
       "...                   ...                       ...                      ...   \n",
       "7995             0.404768                  0.405047                 0.405047   \n",
       "7996             0.404768                  0.405047                 0.405047   \n",
       "7997             0.404768                  0.405047                 0.405047   \n",
       "7998             0.404768                  0.405047                 0.405047   \n",
       "7999             0.404768                  0.405047                 0.405047   \n",
       "\n",
       "      metformin.pioglitazone    change  diabetesMed  \n",
       "0                   0.405047  0.380574     0.355984  \n",
       "1                   0.405047  0.380574     0.355984  \n",
       "2                   0.405047  0.380574     0.421059  \n",
       "3                   0.405047  0.380574     0.421059  \n",
       "4                   0.405047  0.437398     0.421059  \n",
       "...                      ...       ...          ...  \n",
       "7995                0.405047  0.380574     0.421059  \n",
       "7996                0.405047  0.437398     0.421059  \n",
       "7997                0.405047  0.437398     0.421059  \n",
       "7998                0.405047  0.437398     0.421059  \n",
       "7999                0.405047  0.437398     0.421059  \n",
       "\n",
       "[7846 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5ba3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the cat encoder to replace the values from the categories\n",
    "\n",
    "#encode columns with categorical data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "cat_data=cat_data = df.drop(['time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc','readmitted'], axis = 1 )\n",
    "\n",
    "for i in cat_data:\n",
    "    cat_data[i] = le.fit_transform(cat_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653b71a",
   "metadata": {},
   "source": [
    "### Pre-processed numerical data appropriately including handling missing data and justified methods used. Note this may be different for each algorithm you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b4ce979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].replace({\"[70-80)\":75,\n",
    "                         \"[60-70)\":65,\n",
    "                         \"[50-60)\":55,\n",
    "                         \"[80-90)\":85,\n",
    "                         \"[40-50)\":45,\n",
    "                         \"[30-40)\":35,\n",
    "                         \"[90-100)\":95,\n",
    "                         \"[20-30)\":25,\n",
    "                         \"[10-20)\":15,\n",
    "                         \"[0-10)\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d014c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will return all the numeric columns\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "num_data = df.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5470db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = df[\"readmitted\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "277ef186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from time import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c1b99",
   "metadata": {},
   "source": [
    "### Lemmatize the text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e2ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# clean_text is a function to remove tokens like white spaces in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34dcfe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the stop words from the text\n",
    "text_data = {}\n",
    "\n",
    "text_data['Processed_diag_1_desc'] = df.diag_1_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "text_data['Processed_diag_2_desc'] = df.diag_2_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "text_data['Processed_diag_3_desc'] = df.diag_3_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "text_data = pd.DataFrame(text_data)\n",
    "\n",
    "#text_data = pd.concat([Processed_diag_1_desc,Processed_diag_2_desc,Processed_diag_3_desc],axis=1)\n",
    "\n",
    "# Create a new column joining the 3 text columns\n",
    "\n",
    "#df['diag_desc'] = df['diag_1_desc'] + '. ' + df['diag_2_desc'] + '. ' + df['diag_3_desc']\n",
    "\n",
    "#df['Processed_diag_desc'] = df.diag_desc.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c78ea88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the processed dataframes\n",
    "data = pd.concat([cat_data,num_data,text_data,target_data],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75925db9",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "457fe749",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['Processed_diag_1_desc','Processed_diag_2_desc','Processed_diag_3_desc']], df['readmitted'], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25ecd1",
   "metadata": {},
   "source": [
    "#### Apply TfidfVectorizer and Make Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3dea994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the params\n",
    "tfidf_params = dict(sublinear_tf= True, \n",
    "                       #min_df = 5, \n",
    "                       norm= 'l2', \n",
    "                       #ngram_range= (1,2), \n",
    "                       stop_words ='english')\n",
    "\n",
    "# create a Pipeline that will do features transformation then pass to the model\n",
    "\n",
    "cls = Pipeline(steps=[\n",
    "('features', TfidfVectorizer(**tfidf_params)),\n",
    "#('elasticnet', ElasticNet(random_state=0))\n",
    "('clf', LogisticRegression(solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19969a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['Processed_diag_1_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred_model1 = cls.predict(X_test['Processed_diag_1_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2198d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['Processed_diag_2_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred_model2 = cls.predict(X_test['Processed_diag_2_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e273a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['Processed_diag_3_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred_model3 = cls.predict(X_test['Processed_diag_3_desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e835b",
   "metadata": {},
   "source": [
    "### Make new predictions with GBT using the text predictions, categorical and numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e46c3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model1 = pd.DataFrame(y_pred_model1.astype(int),columns = [\"text_model1\"])\n",
    "text_model2 = pd.DataFrame(y_pred_model2.astype(int), columns = [\"text_model2\"])\n",
    "text_model3 = pd.DataFrame(y_pred_model3.astype(int), columns = [\"text_model3\"])\n",
    "\n",
    "num_data_train = pd.DataFrame(num_data.loc[y_train.index], columns = [\"num_data\"])\n",
    "\n",
    "cat_data_train = pd.DataFrame(cat_data.loc[y_train.index], columns = [\"cat_data\"])\n",
    "\n",
    "# join the new inputs\n",
    "X_train = pd.concat([cat_data_train,num_data_train,text_model1,text_model2,text_model3],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "91b5c302",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17540/4142793593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#clf.score(X_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0m\u001b[0;32m    413\u001b[0m                                    dtype=DTYPE, multi_output=True)\n\u001b[0;32m    414\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    721\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
