{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "467c2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from time import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b872e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "df = pd.read_csv('8k_diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d61ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropAndReplaceData(df):\n",
    "\n",
    "    # Drop weight, payer_code, medical_specialty because they have a large number of missing values\n",
    "\n",
    "    df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)\n",
    "\n",
    "    #acetohexamide,examide,citoglipton,Troglitazone, glimepiride.pioglitazone, metformin.rosiglitazone ,metformin.pioglitazone \n",
    "\n",
    "    #In the following columns, there are 2 or 3 classes, however one of them has the majority of values\n",
    "\n",
    "    #Tolbutamide, glipizide.metformin \n",
    "    #- The great majority of the data is NO, 7998. Only 2 are Steady\n",
    "    df = df.drop(['tolbutamide', 'glipizide.metformin'], axis = 1)\n",
    "    #Tolazamide\n",
    "    #- The great majority of the data is NO, 7999. Only 1 is Steady\n",
    "    df = df.drop(['tolazamide'], axis = 1)\n",
    "    #Miglitol\n",
    "    #- The great majority is NO 7997, Steady 2 and Down 1\n",
    "    df = df.drop(['miglitol'], axis = 1)\n",
    "\n",
    "    #Acarbose\n",
    "    #- The majority is NO 7976, Steady 23 and Up 1\n",
    "    df = df.drop(['acarbose'], axis = 1)\n",
    "\n",
    "    #Chlorpropamide\n",
    "    #- No 7990, Steady 9, Up 1\n",
    "    df = df.drop(['chlorpropamide'], axis = 1)\n",
    "\n",
    "    #Nateglinide\n",
    "    #- No 7962, Steady 36, Down 1, Up 1\n",
    "    df = df.drop(['nateglinide'], axis = 1)\n",
    "\n",
    "    #Repaglinide\n",
    "    #- No 7888, Steady 96, Up 11, Down 5\n",
    "    df = df.drop(['repaglinide'], axis = 1)\n",
    "\n",
    "    #acetohexamide has only one value for all rows\n",
    "    df = df.drop(['acetohexamide'], axis =1)\n",
    "\n",
    "\n",
    "    #df = df.replace('?', 'Not Mapped')\n",
    "    df = df.replace('?', 'Missing')\n",
    "\n",
    "\n",
    "    #df['admission_type_id'].isnull().sum()\n",
    "\n",
    "    # I decided to replace the null values in the following columns because the data on these rows could be important\n",
    "\n",
    "    df['admission_type_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "    df['discharge_disposition_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "    df['admission_source_id'].fillna(\"Not Mapped\", inplace = True)\n",
    "\n",
    "    # In the case of the diagnostic description, I prefer to leave it as \"\" but not null. \n",
    "    df['diag_1_desc'].fillna(\"\", inplace = True)\n",
    "    df['diag_2_desc'].fillna(\"\", inplace = True)\n",
    "    df['diag_3_desc'].fillna(\"\", inplace = True)\n",
    "\n",
    "\n",
    "    df['age'] = df['age'].replace({\"[70-80)\":75,\n",
    "                             \"[60-70)\":65,\n",
    "                             \"[50-60)\":55,\n",
    "                             \"[80-90)\":85,\n",
    "                             \"[40-50)\":45,\n",
    "                             \"[30-40)\":35,\n",
    "                             \"[90-100)\":95,\n",
    "                             \"[20-30)\":25,\n",
    "                             \"[10-20)\":15,\n",
    "                             \"[0-10)\":5})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73684137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dropAndReplaceData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc1496d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text is a function to remove tokens like white spaces in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lematization(df):\n",
    "\n",
    "    df['diag_1_desc'] = df.diag_1_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "    df['diag_2_desc'] = df.diag_2_desc.apply(lambda x: clean_text(x))\n",
    "\n",
    "    df['diag_3_desc'] = df.diag_3_desc.apply(lambda x: clean_text(x))\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34e1ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lematization(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdf2821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data preparation for the columns\n",
    "\n",
    "def getCatNumPipeline(categorical_columns, numerical_columns):\n",
    "\n",
    "    cat_encoder = ce.CatBoostEncoder()\n",
    "\n",
    "    tfidf_params = dict(sublinear_tf= True, \n",
    "                           #min_df = 5, \n",
    "                           norm= 'l2', \n",
    "                           #ngram_range= (1,2), \n",
    "                           stop_words ='english')\n",
    "\n",
    "    transformer = [\n",
    "        ('cat_encoder', cat_encoder, categorical_columns), \n",
    "        #('num_scaler', MinMaxScaler(), numerical_columns),\n",
    "        ('num_scaler', StandardScaler(), numerical_columns),\n",
    "\n",
    "        #('tf_idf',TfidfVectorizer(**tfidf_params), text_columns)\n",
    "        ]\n",
    "\n",
    "    col_transform = ColumnTransformer(transformers=transformer)\n",
    "\n",
    "    #text_model = LogisticRegression(solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5)\n",
    "\n",
    "    pipeline = Pipeline(steps=[('prep',col_transform)])\n",
    "    \n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "733c4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into inputs and outputs\n",
    "\n",
    "X = df.drop(['readmitted'],axis =1)\n",
    "y = df['readmitted'].astype(int)\n",
    "\n",
    "# determine categorical and numerical features\n",
    "\n",
    "numerical_columns = X.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns\n",
    "\n",
    "categorical_columns = df.drop(['time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc','readmitted'],axis=1).columns\n",
    "#X.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "text_columns = ['diag_1_desc','diag_2_desc','diag_3_desc']\n",
    "\n",
    "\n",
    "pipeline = getCatNumPipeline(categorical_columns, numerical_columns)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size=0.2)\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_train_cat_num = pipeline.transform(X_train)\n",
    "\n",
    "X_test_cat_num = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584dcff3",
   "metadata": {},
   "source": [
    "TF-IDF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc44318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTextPipeline():\n",
    "\n",
    "    # get the params\n",
    "    tfidf_params = dict(sublinear_tf= True, \n",
    "                           #min_df = 5, \n",
    "                           norm= 'l2', \n",
    "                           #ngram_range= (1,2), \n",
    "                           stop_words ='english')\n",
    "\n",
    "\n",
    "    # create a Pipeline that will do features transformation then pass to the model\n",
    "\n",
    "    cls = Pipeline(steps=[\n",
    "    ('features', TfidfVectorizer(**tfidf_params)),\n",
    "    #('elasticnet', ElasticNet(random_state=0))\n",
    "    ('clf', LogisticRegression(solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5))\n",
    "    ])\n",
    "    \n",
    "    return cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e03e7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = getTextPipeline()\n",
    "\n",
    "def getTextMetamodel(X_train,y_train,X_test):\n",
    "    \n",
    "    # Use clf as a model, fit X_train and y_train\n",
    "    cls.fit(X_train['diag_1_desc'], y_train)\n",
    "\n",
    "    # predicted \n",
    "    text_pred_model1 = cls.predict_proba(X_test['diag_1_desc'])[:,1]\n",
    "    #text_pred_model1 = cls.predict(X_test['Processed_diag_1_desc'])\n",
    "\n",
    "    # Use clf as a model, fit X_train and y_train\n",
    "    cls.fit(X_train['diag_2_desc'], y_train)\n",
    "\n",
    "    # predicted \n",
    "    text_pred_model2 = cls.predict_proba(X_test['diag_2_desc'])[:,1]\n",
    "    #text_pred_model2 = cls.predict(X_test['Processed_diag_2_desc'])\n",
    "\n",
    "    # Use clf as a model, fit X_train and y_train\n",
    "    cls.fit(X_train['diag_3_desc'], y_train)\n",
    "\n",
    "    # predicted \n",
    "    text_pred_model3 = cls.predict_proba(X_test['diag_3_desc'])[:,1]\n",
    "    #text_pred_model3 = cls.predict(X_test['Processed_diag_3_desc'])\n",
    "    \n",
    "    return text_pred_model1,text_pred_model2,text_pred_model3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3be6526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinData(cat_num,text_pred1,text_pred2,text_pred3):\n",
    "    \n",
    "    #numerical_columns and text_columns are \"Global Variables\". There is no need to use them as attributes\n",
    "    \n",
    "    columns = np.append(categorical_columns, numerical_columns)\n",
    "\n",
    "    #df_cat_num = pd.DataFrame(data=X_test_cat_num,columns = columns)\n",
    "\n",
    "    df_cat_num = pd.DataFrame(data=cat_num,columns = columns)\n",
    "\n",
    "    #df_text_pred = pd.DataFrame(np.array([text_pred_model1,text_pred_model2,text_pred_model3]).T,columns = text_columns)\n",
    "    df_text_pred = pd.DataFrame(np.array([text_pred_model1,text_pred_model2,text_pred_model3]).T,columns = text_columns)\n",
    "\n",
    "    X = df_cat_num.join(df_text_pred)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "082ed406",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pred_model1,text_pred_model2,text_pred_model3 = getTextMetamodel(X_train,y_train,X_test)\n",
    "\n",
    "X = joinData(X_test_cat_num,text_pred_model1,text_pred_model2,text_pred_model3)\n",
    "\n",
    "y = y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc1cc2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=9, n_estimators=400,\n",
       "                           random_state=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=400, learning_rate=1.0,max_depth=9, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95ea0c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6375"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19360ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5926066270893857"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "auc_score1 = roc_auc_score(y_test, pred_prob[:,1])\n",
    "\n",
    "auc_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a24bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a45b2cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59719405, 0.56355646, 0.58181204, 0.57133198, 0.49560514,\n",
       "       0.59533469, 0.65984226, 0.61369357, 0.55042792, 0.54740728])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfGBC = GradientBoostingClassifier(n_estimators=40, learning_rate=1.0,max_depth=9, random_state=0)\n",
    "    \n",
    "scores = cross_val_score(clfGBC, X, y, cv=10,scoring='roc_auc')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc8d0001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.637762  , 0.5872211 , 0.67900609, 0.62711291, 0.59001014,\n",
       "       0.62770453, 0.72252056, 0.61511999, 0.64994127, 0.5839906 ])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfRFC = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    \n",
    "scores = cross_val_score(clfRFC, X, y, cv=10,scoring='roc_auc')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9d260908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6155109258557535"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfRFC.fit(X_train, y_train)\n",
    "\n",
    "pred_prob = clfRFC.predict_proba(X_test)\n",
    "\n",
    "auc_score2 = roc_auc_score(y_test, pred_prob[:,1])\n",
    "\n",
    "auc_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27688757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61883029, 0.56659905, 0.65179175, 0.59516565, 0.62964841,\n",
       "       0.65111562, 0.64524249, 0.62426582, 0.65145159, 0.52878   ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfSDGC = SGDClassifier()\n",
    "    \n",
    "scores = cross_val_score(clfSDGC, X, y, cv=10,scoring='roc_auc')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706b304",
   "metadata": {},
   "source": [
    "### Test with 2k File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d80332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scoring = pd.read_csv('2k_diabetes_scoring.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb24c6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['race', 'gender', 'age', 'weight', 'admission_type_id',\n",
       "       'discharge_disposition_id', 'admission_source_id', 'time_in_hospital',\n",
       "       'payer_code', 'medical_specialty', 'num_lab_procedures',\n",
       "       'num_procedures', 'num_medications', 'number_outpatient',\n",
       "       'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3',\n",
       "       'number_diagnoses', 'max_glu_serum', 'A1Cresult', 'metformin',\n",
       "       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
       "       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
       "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
       "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
       "       'glyburide.metformin', 'glipizide.metformin',\n",
       "       'glimepiride.pioglitazone', 'metformin.rosiglitazone',\n",
       "       'metformin.pioglitazone', 'change', 'diabetesMed', 'diag_1_desc',\n",
       "       'diag_2_desc', 'diag_3_desc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scoring.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "108bff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data data for scoring\n",
    "\n",
    "\n",
    "df_scoring = dropAndReplaceData(df_scoring)\n",
    "\n",
    "df_scoring = lematization(df_scoring)\n",
    "\n",
    "#numerical_columns\n",
    "\n",
    "#categorical_columns\n",
    "\n",
    "#text_columns\n",
    "\n",
    "\n",
    "X = df_scoring\n",
    "\n",
    "#pipeline = getCatNumPipeline(categorical_columns, numerical_columns)\n",
    "\n",
    "\n",
    "#pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Encode categorical data and standarize numerical data \n",
    "\n",
    "\n",
    "X_cat_num = pipeline.transform(X)\n",
    "\n",
    "# Reusing cls\n",
    "\n",
    "# predicted \n",
    "text_pred_model1 = cls.predict_proba(X['diag_1_desc'])[:,1]\n",
    "#text_pred_model1 = cls.predict(X_test['Processed_diag_1_desc'])\n",
    "\n",
    "# predicted \n",
    "text_pred_model2 = cls.predict_proba(X['diag_2_desc'])[:,1]\n",
    "#text_pred_model2 = cls.predict(X_test['Processed_diag_2_desc'])\n",
    "\n",
    "# predicted \n",
    "text_pred_model3 = cls.predict_proba(X['diag_3_desc'])[:,1]\n",
    "#text_pred_model3 = cls.predict(X_test['Processed_diag_3_desc'])\n",
    "\n",
    "X = joinData(X_cat_num,text_pred_model1,text_pred_model2,text_pred_model3)\n",
    "\n",
    "#score = clf.score(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = clf.predict_proba(X)[:,1]\n",
    "pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cad628",
   "metadata": {},
   "outputs": [],
   "source": [
    "readmitted=pd.DataFrame(data=pred,columns = ['readmitted'])\n",
    "#df_scoring.join(readmitted).to_csv('freire_diego_pred2.csv')\n",
    "\n",
    "df_scoring.join(readmitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93f995",
   "metadata": {},
   "source": [
    "### 8k train 2k test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('8k_diabetes.csv')\n",
    "\n",
    "df_test = pd.read_csv('2k_diabetes_scoring.csv')\n",
    "\n",
    "\n",
    "df_train = dropAndReplaceData(df_train)\n",
    "\n",
    "df_test = dropAndReplaceData(df_test)\n",
    "\n",
    "df_train = lematization(df_train)\n",
    "\n",
    "df_test = lematization(df_test)\n",
    "\n",
    "numerical_columns = df_train.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns\n",
    "\n",
    "categorical_columns = df_train.drop(['time_in_hospital','num_lab_procedures','num_procedures','num_medications','number_outpatient','number_emergency','number_inpatient','number_diagnoses','diag_1_desc','diag_2_desc','diag_3_desc','readmitted'],axis=1).columns\n",
    "#X.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "text_columns = ['diag_1_desc','diag_2_desc','diag_3_desc']\n",
    "\n",
    "\n",
    "X_train = df_train.drop([\"readmitted\"],axis=1)\n",
    "\n",
    "y_train = df_train[\"readmitted\"].astype(int)\n",
    "\n",
    "X_test = df_test\n",
    "\n",
    "\n",
    "pipeline = getCatNumPipeline(categorical_columns,numerical_columns)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_train_cat_num = pipeline.transform(X_train)\n",
    "\n",
    "X_test_cat_num = pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "# text metamodel\n",
    "\n",
    "#text_pred_model1,text_pred_model2,text_pred_model3 = getTextMetamodel(X_train,y_train,X_train)\n",
    "\n",
    "cls = getTextPipeline()\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['diag_1_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "text_pred_model1 = cls.predict_proba(X_train['diag_1_desc'])[:,1]\n",
    "#text_pred_model1 = cls.predict(X_test['Processed_diag_1_desc'])\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['diag_2_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "text_pred_model2 = cls.predict_proba(X_train['diag_2_desc'])[:,1]\n",
    "#text_pred_model2 = cls.predict(X_test['Processed_diag_2_desc'])\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train['diag_3_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "text_pred_model3 = cls.predict_proba(X_train['diag_3_desc'])[:,1]\n",
    "\n",
    "\n",
    "X_train = joinData(X_train_cat_num,text_pred_model1,text_pred_model2,text_pred_model3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=400, learning_rate=1.0,max_depth=9, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls = getTextPipeline()\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "#cls.fit(X_train['diag_1_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "text_pred_model1 = cls.predict_proba(X_test['diag_1_desc'])[:,1]\n",
    "#text_pred_model1 = cls.predict(X_test['Processed_diag_1_desc'])\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "#cls.fit(X_train['diag_2_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "text_pred_model2 = cls.predict_proba(X_test['diag_2_desc'])[:,1]\n",
    "#text_pred_model2 = cls.predict(X_test['Processed_diag_2_desc'])\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "#cls.fit(X_train['diag_3_desc'], y_train)\n",
    "\n",
    "# predicted \n",
    "text_pred_model3 = cls.predict_proba(X_test['diag_3_desc'])[:,1]\n",
    "\n",
    "\n",
    "X_test = joinData(X_test_cat_num,text_pred_model1,text_pred_model2,text_pred_model3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "readmitted=pd.DataFrame(data=pred,columns = ['readmitted'])\n",
    "df_scoring.join(readmitted).to_csv('freire_diego_pred2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scoring.join(readmitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bb0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
